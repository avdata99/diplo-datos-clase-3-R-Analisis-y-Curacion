---
title: "Práctico 2. Clase 4 de Análisis y Curacion"
output: html_notebook
author: Andrés Vázquez
---

### Diagnosticando Cancer:

* Investigaremos ahora la utilidad del ML para detectar cancer aplicandoel algoritmo kNN a mediciones de biopsias de mujeres, utilizando el  conjunto de datos  "Breast Cancer Winscosin Diagnostic" del UCI ML Repository <http://archive.ics.uci.edu/ml> que incluye 569 ejemplos de biopsias, en cada una se midieron 32 features  (diferentes caracteristicas de las nucleos celulares) y el diagnostico codificado como M (Maligno) o B (Benigno).

```{r echo=TRUE}
data <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=FALSE)
data <- data[-1]
str(data)
```


### Entendiendo los datos:
Independientemente del metodo de ML aplicado, las variables de identificacion **deben** ser excluidas. No hacerlo puede llevar a hallazgos erroneos a causa de que la identificacion puede ser utilizada para predecir muy bien.
La siguiente variable, el diagnostico es de particular interes, por que es lo que se quiere predecir.


```{r echo=TRUE}
table(data$V2)
```

Ya que estamos miremos el resto de las variables, sus rangos etc.

```{r echo=TRUE}
summary(data)
```

***
### Transformacion de los datos:

Necesitamos crear una funcion normalizacion en R:

```{r echo=TRUE}
normalize <- function(x) {
  return ((x-min(x))/(max(x)-min(x)))
}
```

Despues de ejecutar el codigo previo, la funcion esta disponible para sus uso. Veamos si funciona en algunos vectores.

```{r echo=TRUE}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
```

No podemos aplicar la funcion a los features numericos del dataframe directamente.

### Transformacion de los datos:

La funcion lapply() de R toma una lista y aplica una funcion a cada elemento de la lista.
Uso la funcion ZSCORE nueva además de _normalize_

```{r echo=TRUE}
data_n <- as.data.frame(lapply(data[2:31], normalize))
summary(data_n$V3)
summary(data_n$V8)
```

Bingo!  En ausencia de nuevos datos de laboratorio, vamos a simular este escenario dividiendo 
nuestros datos en una **muestra de entrenamiento**  que usaremos para construir el modelo kNN 
y una **muestra de validacion** que usaremos para medir la precision predictiva del mismo.

***
### Entrenando un clasificador:

Notese que dichos conjuntos de datos deben ser representativos del conjunto de datos, i.e. **metodos de muestreo aleatorios**!

```{r echo=TRUE}
data_train <- data_n[1:469, ]
data_test  <- data_n[470:569, ] 
```

Excluimos la variable objetivo (Benigno/Maligno), pero necesitamos guardar estos factores en vectores!

```{r echo=TRUE}
data_train_labels <- data[1:469, 1]
data_test_labels  <- data[470:569, 1]
```

Para el algoritmo kNN la fase de entrenamiento no involucra construir un modelo, para
clasificar nuestros datos de validacion utilizaremos el paquete class, instalarlo ia!

La datos de validacion son clasificados tomando los votos entre los k vecinos mas cercanos
del conjunto de entrenamiento. Si hay empate se decide aleatoriamente. Entonces usamos
la funcion knn() para clasificar.

### Evaluando la performance del modelo.


```{r echo=TRUE}
library(class)
data_test_pred <- knn(train=data_train, test=data_test, cl=data_train_labels, k=21)
```

El siguiente paso en el proceso es evaluar como las clases predichas  en data_test_pred
se condicen con los valores verdaderos en el vector data_test_labels.

```{r echo=TRUE}
library(gmodels)
CrossTable(x=data_test_labels, y=data_test_pred, prop.chisq = FALSE)
```


### Ejercicios.

Los valores correctos estan en la diagonal de la matriz, 98% de precision para unas pocas lineas de R!
Probar todo con z-score

#### Obtener datos
```{r}
data <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=FALSE)
data <- data[-1]

```

+ Mejore el rendimiento utilizando una normalizacion con z-scores provista por la funcion scale() de R.
```{r}
# MI FUNCION Z-SCORE
zscore <- function(x) {
  return ((x-mean(x)) / sd(x))
}

# # PROBANDO MI Z-SCORE
# zscore(c(1,2,3,4,5))
# zscore(c(10,20,30,40,50))
# 
# # PROBANDO MI Z-SCORE con outliers
# zscore(c(1,2,3,4,5, 1000))
# zscore(c(10,20,30,40,50, 10000))

data_n_z <- as.data.frame(lapply(data[2:31], zscore))
# summary(data_n_z$V3)
# summary(data_n_z$V8)

data_train_z <- data_n_z[1:469, ]
data_test_z  <- data_n_z[470:569, ] 

data_train_labels <- data[1:469, 1]
data_test_labels  <- data[470:569, 1]

data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=21)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

+ Pruebe algunos valores alternativos de k=1, 5,  11, 15, 21 y seleccione el mejor valor de k.

```{r}
data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=1)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

```{r}
data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=5)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

```{r}
data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=11)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

```{r}
data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=15)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

```{r}
data_test_pred_z <- knn(train=data_train_z, test=data_test_z, cl=data_train_labels, k=21)
CrossTable(x=data_test_labels, y=data_test_pred_z, prop.chisq = FALSE)
```

+ mientras termina su merecido cafe verifique si el resultado cambia utilizando paciente elegidos aleatoriamente para el conjunto de validacion.

  
***
### Una breve excursion en dimensiones mas altas:

**Ajustar** una funcion (como el modelo lineal) nos permite generalizar nuestra estima de la funcion 
densidad mas alla de los puntos medidos, pero no nos permite manejar otras generalizaciones bien.

Un primer problema es que tipo de funciones necesitamos para representar, consideren por ejemplo unos puntos distribuidos en una superficie de baja dimensionalidad en un espacio de dimensionalidad mayor, por 
ejemplo una hoja de cuaderno en 3D. En la direccion perpendicular a la superficie la distribucion es muy 
angosta, algo que es dificil de expandir con funciones bases trigonometricas o polinomicas.

Este problema es muy comun, en un espacio de features de alta dimensionalidad toda distribucion se vuelve superficial. Para entender este concepto, considere el
volumen de una hiper esfera de radio $r$ en un espacio de dimension $d$:

$$V(r)=\frac{\pi^{d/2}r^{d}}{(d/2)!}$$ 

los factoriales no enteros estan definidos por: $\Gamma(n+1)=n!$. 

***
### y los problemas de las distribuciones alli.

Ahora miremos a la fraccion de volumen que ocupa una cascara de ancho $\epsilon$, comparada al volumen total
de la esfera, en el limite cuando  $$d \rightarrow \infty$$: 
$$
\frac{V(r)-V(r-\epsilon)}{V(r)}=\frac{r^{d}-(r-\epsilon)^{d}}{r^{d}}=1-(1-\frac{\epsilon}{r})^d=1
$$
Cuando d crece, todo el volumen esta en una cascara fina sobre la superficie. 

Ahora considere lo que implica esto para una una coleccion de puntos que provienen de una distribucion:
puntos tipicos provienen de una distribucion no de los bordes, pero en un espacio de alta dimensionalidad
las distribuciones son esencialmente bordes y los analisis estaran dominados por los efectos de borde.

Otro punto importante es puede que no podamos muestrear bien la distribucion alli.

***
### Using mixture models for clustering

Dados los problemas de estimacion mencionados, una posible solucion es proponer por ejemplo poner alguna
distribucion (Gaussiana por ejemplo) en cada punto medido, esto se conoce como **Kernel density estimation**.

Una mejor aproximacion seria encontrar lugares interesantes donde poner un numero chico (respecto al numero de datos) donde poner algunas funciones locales que modelen bien su vecindad.  Esto se conoce como **mixture models**.

Esto esta conectado con el problema de partir un conjunto de datos aglomerando o **clustering**, que es un ejemplo de **aprendizaje no supervisado**. A diferencia de los metodos de fiteo con una funcion distribucion determinada, el algoritmo debe aprender por si mismo donde estan esos lugares interesantes en el conjunto de datos.

Importantes aplicaciones de estos algoritmos se encuentran en la compresion de datos, la deteccion de
outliers y crear clasificadores generativos, donde se modela cada  densidad de clase $p(x|y = c)$ por una
mixtura.

***
### Mixturas de Gausianas en D dimensiones:

un modelo de mixturas puede escribirse factorizando la densidad sobre multiples Gaussianas:

$$p(\bar{x})=\sum_{m=1}^{M}p(\bar{x}, c_{m})=\sum_{m=1}^{M}p(\bar{x}|c_{m})p(c_{m})$$


$$p(\bar{x})=\sum_{m=1}^{M} \frac{|C_{m}^{-1}|^{1/2}}{(2 \pi)^{D/2}} \exp[{-(\bar{x}-\bar{\mu})^{T}.C_{M}^{-1}.(\bar{x}-(\bar{\mu}))/2}]p(c_{m})$$

donde $|.|^{1/2}$ es la raiz cuadrada del determinante, y $c_{m}$ se refieren a la $m$-esima Gaussiana con media $\mu_{m}$ y matriz de covarianza $C_{m}$.
El desafio es por supuesto encontrar esos parametros!

***
### Mixturas de Gausianas en D dimensiones:


Si tenemos una sola Gaussiana, el valor medio $\mu$ puede ser estimado simplemente promediando los los datos:
$$ \bar{\mu}=\int_{-\infty}^{\infty} \bar{x} p(\bar{x}) d\bar{x} \simeq \frac{1}{N} \sum_{n=1}^{N} \bar{x}_{n}$$

dado que una integral sobre una funcion densidad de probabilidad puede aproximarse por una suma de variables extraidas de la distribucion
(importante recordar). Todavia no la conocemos a la funcion distribucion, pero por definicion es de donde nuestro conjunto de datos fue extraido.

La idea puede extenderse a mas Gaussianas reconociendo que la $m$esima media es la integral respecto a la distribucion condicional:
$$\bar{\mu}=\int \bar{x} p(\bar{x} | c_{m}) d\bar{x} = \int \bar{x} \frac{p(c_{m}|\bar{x})}{p(c_m)}p(\bar{x})d(\bar{x}) \simeq \frac{1}{Np(c_{m})}\sum_{n=1}^{N} \bar{x}_{n} p(c_{m} | \bar{x}_{n})$$

***
### Mixturas de Gausianas en D dimensiones:

de forma similar se puede encontrar que la matriz de covarianza es:

$$ C_{m} \simeq \frac{1}{N p(c_{m})} \sum_{n+1}^{N} (\bar{x}_{n}-\bar{\mu}_{m})(\bar{x}_{n}-\bar{\mu}_{m})^{T} p(c_{m}| x_{n})$$

y los pesos en la expansion por,

$$ p(c_{m})=\int_{-\infty}^{\infty} p(\bar{x}, c_{m}) d\bar{x}= \int_{-\infty}^{\infty} p(c_{m} | \bar{x}) p(\bar{x}) d{\bar{x}} \simeq \frac{1}{N} \sum_{i=1}^{N} p(c_{m} | \bar{x}_{n})$$
Pero, como encontramos la probabilidad posterior $p(c_{m}|\bar{x})$ utilizada en estas sumas? Por definicion es:

***
### El Algoritmo EM

$$ p(c_{m}| \bar{x})=\frac{p(\bar{x}, c_{m})}{p(\bar{x})}=\frac{p(\bar{x}| c_{m})p(c_{m})}{\sum_{m=1}^{M} p(\bar{x}|c_{m})p(c_{m})}
$$

lo cual puede calcularse segun el modelo inicial propuesto. Esto puede sonar a un razonamiento circular y lo es!
Las probabilidades de los puntos pueden ser calculados si conocemos los parametros de las distribuciones 
(medias, varianzas, pesos) y los parametros pueden ser encontrados si conocemos las probabilidades.

Como comenzamos no conociendo nada, comenzamos suponiendo los parametros aleatoriamente y vamos y venimos iterativamente
computando iterativamente las probabilidades y los parametros.

***
### El algoritmo EM y sus parientes:

Calculando una distribucion esperada dados los parametros y luego encontrando los parametros mas
probables dada una distribucion, es llamado el algoritmo de maximizacion de expectacion EM y este
converge a una distribucion de maximo likelihood comenzando con una semilla aleatoria.

Las Gaussianas pueden por ejemplo ser inicializadas con medias aleatorias y varianzas lo suficientemente 
grandes como para "sentir" el conjunto de datos. El problema es que las Gausianas captura solo proximidad,
el problema surge cuando hay superposicion de las mismas o alguna colapsa.

Una mejor alternativa es basar la expansion de la funcion distribucion alrededor de modelos que pueden capturar
mejor la complejidad localmente. Esta poderosa idea ha llevado ha desarrollar diferentes algoritmos como:
**redes Bayesianas**, **mixtures of experts** o **cluster wigthed models**.

***
### El algoritmo EM, una aplicacion:

```{r}
# Plot our dataset.
plot(iris[, 1:4], col = iris$Species, pch = 18, main = "Fisher's Iris Dataset")
```


***
### El algoritmo EM, una aplicacion:

```{r}
# Mclust comes with a method of hierarchical clustering. 
library(mclust)
# initialize 3 different classes.
initialk <- mclust::hc(data = iris, modelName = "EII")
initialk <- mclust::hclass(initialk, 3)
```

Ahora se utilizan las ecuaciones definidas anteriormente para calcular

```{r echo=TRUE}
# Select 4 continuous variables and look for three distinct groups.
mcl.model <- Mclust(iris[, 1:4], 3)
# Plot our results.
plot(mcl.model, what = "classification", main = "Mclust Classification")
```


***
### Mixturas de expertos:

Tambien podemos usar modelos de mixturas para crear modelos discriminativos de clasificacion y regresion.
Por ejemplo consideren los siguientes datos, parece una buena idea usar tres modelos diferentes de regresion lineal,
cada una aplicada a diferentes partes del espacio de inputs. Esto puede modelarse si permitimos
que los pesos de la mixtura y las densidades sean dependientes del input.

![](./mix.png)


***
### un poco mas sobre Mixturas:

La idea en una mixtura de expertos es que cada submodelo es considerado un **experto** en cierta region  del espacio de los inputs.
La funcion:  $p(z_{i} = k|x_{i}, \theta)$ is llamada funcion gatillo y decide que experto usar en cada rango segun:
 $p(y_{i}|x_{i}, \theta)=\sum_{k} p(z_{i}=k|x_{i}, \theta) p(y_{i}|x_{i}, z_{i}=k, \theta)$.

Resulta claro que podemos "pegar" cualquier **arquitectura** como un experto. Por ejemplo a veces se utilizan redes neuronales 
para representar los expertos y las funciones gatillos, el resultado es conocido como **mixture density network**. Son modelos lentos
para entrenar pero muy flexibles.

Las mixturas de expertos son muy utiles para resolver **problemas de inversion** donde hay que invertir un mapeo de muchos a uno.
Un ejemplo tipico aparece en robotica, donde la localizacion de una mano (robotica) $y$ es unicamente determinada por los diferentes
angulos $x$ de las articulaciones de un brazo robot. Sin embargo, para una dada ubicacion y, hay muchas combinaciones de las
articulaciones $x$, por lo que el mapeo inverso  $x = f^{-1}(y)$ no es unico. Otro ejemplo seria en la identificacion posicional
de gente en video, por los problemas de auto ocultamiento etc. 

***
### Clustering hard K-means:

Una variante particular del algoritmo EM para GMMs es conocido como el **Algoritmo K-means**.
Consideremos un modelo de mixturas Gaussianas en el cual asumimos que:  $\sum_{k} =\sigma^{2} \mathbb{I}_{D}$
y  $\pi_{k} = 1/K$ estan fijos,  por lo que solamente los centros de los cumulos, $\mu_{k} \in \mathbb{R}^{D}$, tienen que 
ser estimados.

Si consideramos la aproximacion de $p(z_{i}=k|x_{i}, \theta) \sim \mathbb{I}(k=z_{i}^{*})$,como $z_{i}^{*}= max_{k} p(z_{i}=k | x_{i}, \theta)$.
obtenemos el llamado **hard K-means**, donde estamos asignando cada punto de datos a un cumulo.
Dado que asumimos una matriz de covarianza esferica (en el espacio de features) para cada cumulo, el cumulo mas
probable para cada dato for $x_{i}$ puede ser computado encontrando el cumulo propuesto mas cercano en cada paso.

En cada paso, debemos encontrar las distancias entre los $N$ puntos de datos y los $K$ centros de los cumulos
lo que lleva un tiempo de orden $NKD$.Sin embargo esto puede acelerarse.
Dada las asignaciones duras, en cada paso puedo recalcular el centro de los cumulos computando
la media de todos los puntos asignados al mismo:  $\mu_{k}=\frac{1}{N} \sum_{i:z_{i}=k} x_{i}$.

***
### ejemplo de K-means:

```{r echo=TRUE}
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
```

***
### ejemplo de K-means:

```{r}
table(irisCluster$cluster, iris$Species)
```

```{r}
library(ggplot2)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
```


***
## Clustering soft K-means:


Cuando computamos la  $p(z_{i} = k|x_{i}, \theta)$, la probabilidad posterior que el punto $i$ pertenezca al cumulo $k$, esta cantidad
se puede pensar como la **responsabilidad** del cumulo $k$ por el punto $i$ la cual puede calcularse siguiendo la regla de Bayes
: $r_{ik}=p(z_{i}=k|x_{i}, \theta)=\frac{p(z_{i}=k|\theta )p(x_{i}|z_{i}=k, \theta)}{\sum_{j=1}^{K} p(z_{i}=j|\theta)p(x_{i}|z_{i}=j, \theta)}$

Este procedimiento es llamado  **soft clustering**, y permite una mejor clasificacion automatica.  Por ejemplo usando una version
binarizada del conjunto de carateres digitalizados MNIST, se puede hacer un clustering con $k=10$ y visualizar los centroides, tal 
como vemos el metodo descubre correctamente algunas clases de digitos, pero otros no (discutir).

![](./handw.png)




***
### Some references and useful material:

Por mas detalles consultar el siguiente libro <http://www.inference.org.uk/itprnn/book.html>.

Very useful free R books with code could be found at: <https://bookdown.org/>, and <https://www.tidyverse.org/>
A nice blog at <https://www.r-bloggers.com>, good tutorials at <https://resources.rstudio.com/webinars>.

Vale la pena mencionar tambien la posibilidad de llamar de python desde R  utilizando <https://rstudio.github.io/reticulate/>
o trasnferir dataframes a pandas y viceversa con <https://github.com/wesm/feather> y el uso de ambos lenguajes en sistemas
distribuidos & cloud.

***
### Weapons of math destruction:

Recuerde que los modelos por su naturaleza son simplificaciones, y cuando lo creamos tomamos decisiones respecto
que es importante incluir (muchas veces en el proceso de seleccion de muestras) por lo que podemos generar enormes
puntos ciegos. Tenga en cuenta los posibles sesgos en los conjuntos de datos, pq todo algoritmo los perpetuara y amplificara.

El reciente caso de Cambridge Analytics deberia llamar vuestra atencion de los posibles mal usos
de las tecnologias que desarrollamos, o los problemas generados por el algoritmic trading, i.e the flash crack etc: <https://www.ted.com/talks/carole_cadwalladr_facebook_s_role_in_brexit_and_the_threat_to_democracy?language=en>

***
## La Ciencia de Datos es multidisciplinaria:

![](./Data_Science_VD.png)

***
## Practico 2: Entregar un Rmd donde se:

- Elija un dataset clasificado de su preferencia y area (domain expertise), aplique un metodo de clustering y/o mixtura de Gaussianas en el mismo.

- Investigue los resultados en el meta parametro $K$ numero de cumulos e investigue posibles procesos de seleccion del mismo.

- Elabore un resumen, y selecione un mejor valor segun el/los criterios aplicados, discuta el significado de los cumulos
encontrados. 

- Comente la influencia de la normalizacion de los datos en los resultados del clustering.